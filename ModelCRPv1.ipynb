{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMOMMklfQEqQ4j/efsEpdIF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nguyen-nhat-mai/object_detection/blob/model_maxime/ModelCRPv1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMplOsdzMINB"
      },
      "outputs": [],
      "source": [
        "!pip install google.colab\n",
        "from google.colab import drive\n",
        "drive = drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -qr https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt  # install dependencies"
      ],
      "metadata": {
        "id": "14eLa3IGMSgu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f9ec7d1-f2e3-49d4-9a7e-62e93c7fe26e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.9.0 requires jedi>=0.10, which is not installed.\n",
            "cvxpy 1.2.3 requires setuptools<=64.0.2, but you have setuptools 67.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "%matplotlib inline\n",
        "import os, re, sys, random, shutil, cv2\n",
        "from datetime import datetime\n",
        "\n",
        "import torch\n",
        "from torchvision import models\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torchsummary\n",
        "\n",
        "from utils.loss import ComputeLoss"
      ],
      "metadata": {
        "id": "J8WcgchgMV1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True, classes = 14) #channels = 1) we do not need to chose it. it automatically rescales grey to rgb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryNCpkzXMtuH",
        "outputId": "2b2fe412-cfe6-45e7-b634-91bdc8c70163"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/ultralytics_yolov5_master\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m YOLOv5 requirement \"setuptools>=65.5.1\" not found, attempting AutoUpdate...\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: setuptools>=65.5.1 in /usr/local/lib/python3.8/dist-packages (67.4.0)\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m 1 package updated per /root/.cache/torch/hub/ultralytics_yolov5_master/requirements.txt\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m ‚ö†Ô∏è \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n",
            "YOLOv5 üöÄ 2023-3-2 Python-3.8.10 torch-1.13.1+cu116 CPU\n",
            "\n",
            "Overriding model.yaml nc=80 with nc=14\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
            "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1     51243  models.yolo.Detect                      [14, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
            "YOLOv5s summary: 214 layers, 7057387 parameters, 7057387 gradients, 16.1 GFLOPs\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Learn how to use these parameters\n",
        "model.conf = 0.25  # NMS confidence threshold\n",
        "model.iou = 0.45  # NMS IoU threshold\n",
        "model.agnostic = False  # NMS class-agnostic\n",
        "model.multi_label = False  # NMS multiple labels per box\n",
        "model.classes = None  # (optional list) filter by class, i.e. = [0, 15, 16] for COCO persons, cats and dogs\n",
        "model.max_det = 10  # maximum number of detections per image\n",
        "model.amp = False  # Automatic Mixed Precision (AMP) inference\n"
      ],
      "metadata": {
        "id": "6jHf7gakVZtI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose the hyperparameters for training: \n",
        "num_epochs = 10\n",
        "batch_size = 16\n",
        "lr = 1e-3\n",
        "\n",
        "# Use mean squared loss function \n",
        "criterion = ComputeLoss(model = model)\n",
        "\n",
        "# It is initialized on our model\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "net_args = {\n",
        "    \"dataset\": \"LabeledAirportMarch2023\",\n",
        "    \"learning_rate\": lr,\n",
        "    \"optimizer\": \"SGD\"\n",
        "            }"
      ],
      "metadata": {
        "id": "qQmrbcI1xrFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To see what layers we have\n",
        "for layer in model.children():\n",
        "  print(layer)\n",
        "\n",
        "#can also use list(model.modules())"
      ],
      "metadata": {
        "id": "vyoq4a05Nkwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Device used: {device}')"
      ],
      "metadata": {
        "id": "p4Ux2Ji4ZArJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4ac952e-970a-4c22-cf7c-114a5ffa1498"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device used: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Use of wandb to follow the changes\n",
        "!pip install wandb -qU\n",
        "import wandb\n",
        "def wandb_connect():\n",
        "    wandb_api_key_label = \"wandb_api_key\"\n",
        "    wandb_api_key = \"9855d20a7918a7c88f92b87c467fb3cc36101f41\" # API key from WandB interface\n",
        "\n",
        "    wandb_conx = wandb.login(key = wandb_api_key)\n",
        "    print(f\"Connected to Wandb online interface : {wandb_conx}\")\n",
        "\n",
        "wandb_connect()"
      ],
      "metadata": {
        "id": "3kZayR9BxWZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_loop(model, epochs, train_dataloader, val_dataloader, optimizer, criterion, device = \"cpu\"):\n",
        "    total_train_losses = []\n",
        "    total_val_losses = []\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in tqdm(range(1,epochs+1)):\n",
        "        #print(f'epoch: {epoch}')\n",
        "        ##TRAINING##\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        trn_lbl = torch.Tensor([])\n",
        "        trn_preds = torch.Tensor([])\n",
        "        for i, batch, in enumerate(train_dataloader):\n",
        "            img_batch, lbl_batch = batch\n",
        "            trn_lbl=torch.cat((trn_lbl, lbl_batch))\n",
        "            img_batch, lbl_batch = img_batch.to(device), lbl_batch.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(img_batch).to(device)\n",
        "            trn_preds=torch.cat((trn_preds, logits.argmax(1).cpu()))\n",
        "\n",
        "                        \n",
        "            # One-hot encoding or labels so as to calculate MSE error:\n",
        "            labels_one_hot = torch.FloatTensor(lbl_batch.shape[0], 10).to(device)\n",
        "            labels_one_hot.zero_()\n",
        "            labels_one_hot.scatter_(1, lbl_batch.view(-1, 1), 1)\n",
        "\n",
        "            loss=criterion(logits, labels_one_hot)\n",
        "            wandb.log({\"train_loss\":loss.item()}) # log the training loss at each batch\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "        train_loss_mean = np.mean(train_losses)\n",
        "        total_train_losses.append(train_loss_mean)\n",
        "\n",
        "        ##VALIDATION##\n",
        "        model.eval()\n",
        "        val_losses = []\n",
        "        val_lbl = torch.Tensor([])\n",
        "        val_preds = torch.Tensor([])\n",
        "        with torch.no_grad():\n",
        "\n",
        "            for i, batch, in enumerate(val_dataloader):\n",
        "                img_batch, lbl_batch = batch\n",
        "                val_lbl=torch.cat((val_lbl, lbl_batch))\n",
        "                img_batch, lbl_batch = img_batch.to(device), lbl_batch.to(device)\n",
        "\n",
        "\n",
        "                # One-hot encoding or labels so as to calculate MSE error:\n",
        "\n",
        "                labels_one_hot = torch.FloatTensor(lbl_batch.shape[0], 10).to(device)\n",
        "                labels_one_hot.zero_()\n",
        "                labels_one_hot.scatter_(1, lbl_batch.view(-1, 1), 1)\n",
        "\n",
        "                logits=model(img_batch).to(device)\n",
        "\n",
        "                val_preds=torch.cat((val_preds, logits.argmax(1).cpu()))\n",
        "                loss=criterion(logits, labels_one_hot)\n",
        "\n",
        "                val_losses.append(loss.item())\n",
        "                wandb.log({\"val_loss\":loss.item()})\n",
        "\n",
        "        val_acc=accuracy(val_preds, val_lbl)\n",
        "        val_loss_mean = np.mean(val_losses)\n",
        "        wandb.log({\"train_acc\":train_acc, \"val_acc\":val_acc}) # log the train & val accuracy and the val loss at each epoch\n",
        "        total_val_losses.append(val_loss_mean)"
      ],
      "metadata": {
        "id": "dU_lu4MNxHap"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}