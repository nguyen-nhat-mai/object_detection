{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "bEIxLGWmBTmQ"
      ],
      "authorship_tag": "ABX9TyOdyNrI5dr9DlKsWbEqzbRq"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Model setup and initial installations"
      ],
      "metadata": {
        "id": "Q5PCeRpVHI_J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TMplOsdzMINB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c21d8f56-6a17-4f2a-aa8b-364ce2b0c642"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hMounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "!pip install -q google.colab\n",
        "from google.colab import drive\n",
        "drive = drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Installing the yolov5 setup\n",
        "!git clone https://github.com/ultralytics/yolov5  # clone\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt  # install\n",
        "\n",
        "import torch\n",
        "import os\n",
        "import utils\n",
        "display = utils.notebook_init()  # checks"
      ],
      "metadata": {
        "id": "14eLa3IGMSgu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cc807b3-601e-491a-8957-6bde14e5758c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 🚀 v7.0-120-g3e55763 Python-3.9.16 torch-1.13.1+cu116 CUDA:0 (Tesla T4, 15102MiB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete ✅ (2 CPUs, 12.7 GB RAM, 25.5/166.8 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSaUTUOFpphw",
        "outputId": "9d9c8ca0-fd81-4feb-abc8-40f05d1866c6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete. Using torch 1.13.1+cu116 (Tesla T4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Download and preparation of the dataset\n",
        "We need a dataset like that:\n",
        "\n",
        "3 folders train, val and test \n",
        "\n",
        "In each of these folder, a folder labels and a folder images"
      ],
      "metadata": {
        "id": "ptU4VtlDrqwx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Copy the dataset to the good location\n",
        "newpath = '/content/yolov5/datasets' \n",
        "if not os.path.exists(newpath):\n",
        "    os.makedirs(newpath)\n",
        "\n",
        "#/content/drive/MyDrive/CRPSharedFolder/CRP__Dataset_Repartition = the path of your dataset\n",
        "%cp -R /content/drive/MyDrive/CRPSharedFolder/CRP__Dataset_Repartition /content/yolov5/datasets/airportscanner "
      ],
      "metadata": {
        "id": "qz8OaV152bPH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Download the custom hyperparameters file (you can change the hyperparameters by modifying this file)\n",
        "!wget -P /content/yolov5/datasets/airportscanner https://raw.githubusercontent.com/nguyen-nhat-mai/object_detection/model_maxime/hyp.custom.airportscanner.yaml\n",
        "#Download the information file regarding the dataset. It describes the classes as well as the train/val/test paths\n",
        "!wget -P /content/yolov5/datasets/airportscanner https://raw.githubusercontent.com/nguyen-nhat-mai/object_detection/model_maxime/dataset.airportscanner.yaml"
      ],
      "metadata": {
        "id": "9gEWa6gl_NmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Aquarium dataset"
      ],
      "metadata": {
        "id": "bEIxLGWmBTmQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Aquarium dataset to test the pipeline\n",
        "!pip -q install roboflow\n",
        "from roboflow import Roboflow\n",
        "newpath = '/content/yolov5/datasets/aquarium' \n",
        "if not os.path.exists(newpath):\n",
        "    os.makedirs(newpath)\n",
        "%cd '/content/yolov5/datasets/aquarium'\n",
        "!curl -L \"https://public.roboflow.com/ds/iE7NSICXKn?key=0OoOirRq2N\" > roboflow.zip; unzip roboflow.zip; rm roboflow.zip"
      ],
      "metadata": {
        "id": "YJ1RnNKHEfkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/yolov5\n",
        "!python train.py \\\n",
        "--img 416 \\\n",
        "--batch 32 \\\n",
        "--epochs 1 \\\n",
        "--data /content/yolov5/datasets/aquarium/data.yaml \\\n",
        "--weights yolov5s.pt \\\n",
        "--cache \\\n",
        "--save-period 1 \\\n",
        "--bbox_interval 1 \\\n",
        "--optimizer Adam #\\\n",
        "#--resume #if you want to resume training\n",
        "\n",
        "#do not forgot to save your best model after training!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjqcEaWeBbyJ",
        "outputId": "9c380235-7d98-41c7-8ecb-da22e0037ae2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/yolov5\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=/content/yolov5/datasets/aquarium/data.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=1, batch_size=32, imgsz=416, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=Adam, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=1, artifact_alias=latest\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ✅\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/yolov5/train.py\", line 640, in <module>\n",
            "    main(opt)\n",
            "  File \"/content/yolov5/train.py\", line 504, in main\n",
            "    check_file(opt.data), check_yaml(opt.cfg), check_yaml(opt.hyp), str(opt.weights), str(opt.project)  # checks\n",
            "  File \"/content/yolov5/utils/general.py\", line 491, in check_file\n",
            "    assert len(files), f'File not found: {file}'  # assert file was found\n",
            "AssertionError: File not found: /content/yolov5/datasets/aquarium/data.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training"
      ],
      "metadata": {
        "id": "VOqXbCGMTEmp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/yolov5\n",
        "!python train.py \\\n",
        "--img 416 \\\n",
        "--batch 32 \\\n",
        "--epochs 1 \\\n",
        "--data /content/yolov5/datasets/airportscanner/dataset.airportscanner.yaml \\\n",
        "--weights yolov5s.pt \\\n",
        "--cache \\\n",
        "--save-period 1 \\\n",
        "--bbox_interval 1 \\\n",
        "--optimizer Adam \\\n",
        "--hyp /content/yolov5/datasets/airportscanner/hyp.custom.airportscanner.yaml #\\ #to fix\n",
        "#--resume #if you want to resume training\n",
        "\n",
        "#Do not forgot to save your best model after training!\n",
        "#Note that lots of plots, predicted images examples and other can be found in the runs/train folder\n"
      ],
      "metadata": {
        "id": "ehXGwSSwB-ny",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a7ee54b-a478-4baa-e195-1a1efdbc55f7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/yolov5\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=/content/yolov5/datasets/airportscanner/dataset.airportscanner.yaml, hyp=/content/yolov5/datasets/airportscanner/hyp.custom.airportscanner.yaml, epochs=1, batch_size=32, imgsz=416, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=Adam, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=1, artifact_alias=latest\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ✅\n",
            "YOLOv5 🚀 v7.0-120-g3e55763 Python-3.9.16 torch-1.13.1+cu116 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.1, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, anchors=3, fl_gamma=1.5, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=15, translate=0.1, scale=0.5, shear=0.005, perspective=5e-05, flipud=0.0, fliplr=0.5, mosaic=0.5, mixup=0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n",
            "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "2023-03-22 19:07:25.525412: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-03-22 19:07:26.799892: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.9/dist-packages/cv2/../../lib64:/usr/local/lib/python3.9/dist-packages/cv2/../../lib64:/usr/lib64-nvidia\n",
            "2023-03-22 19:07:26.800023: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.9/dist-packages/cv2/../../lib64:/usr/local/lib/python3.9/dist-packages/cv2/../../lib64:/usr/lib64-nvidia\n",
            "2023-03-22 19:07:26.800046: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "Overriding model.yaml nc=80 with nc=14\n",
            "Overriding model.yaml anchors with anchors=3\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
            "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1     51243  models.yolo.Detect                      [14, [[0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5]], [128, 256, 512]]\n",
            "Model summary: 214 layers, 7057387 parameters, 7057387 gradients, 16.1 GFLOPs\n",
            "\n",
            "Transferred 342/349 items from yolov5s.pt\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/yolov5/datasets/airportscanner/train/labels.cache... 262 images, 5 backgrounds, 0 corrupt: 100% 262/262 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.1GB ram): 100% 262/262 [00:04<00:00, 58.09it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/yolov5/datasets/airportscanner/valid/labels.cache... 193 images, 25 backgrounds, 0 corrupt: 100% 193/193 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.1GB ram): 100% 193/193 [00:07<00:00, 25.30it/s]\n",
            "\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m0.02 anchors/target, 0.008 Best Possible Recall (BPR). Anchors are a poor fit to dataset ⚠️, attempting to improve...\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 9 anchors on 519 points...\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7155: 100% 1000/1000 [00:00<00:00, 1899.82it/s]\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.25: 0.9788 best possible recall, 5.08 anchors past thr\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=9, img_size=416, metric_all=0.338/0.719-mean/best, past_thr=0.487-mean: 28,46, 73,51, 92,149, 163,92, 120,263, 161,389, 314,201, 257,363, 373,333\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ✅ (optional: update model *.yaml to use these anchors in the future)\n",
            "Plotting labels to runs/train/exp3/labels.jpg... \n",
            "Image sizes 416 train, 416 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/train/exp3\u001b[0m\n",
            "Starting training for 1 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        0/0      3.23G    0.09883   0.003285    0.01259         29        416: 100% 9/9 [00:07<00:00,  1.17it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0% 0/4 [00:00<?, ?it/s]WARNING ⚠️ NMS time limit 3.700s exceeded\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  25% 1/4 [00:04<00:12,  4.01s/it]WARNING ⚠️ NMS time limit 3.700s exceeded\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50% 2/4 [00:08<00:08,  4.03s/it]WARNING ⚠️ NMS time limit 3.700s exceeded\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:12<00:00,  3.06s/it]\n",
            "                   all        193        472   0.000659     0.0549   0.000723   0.000224\n",
            "\n",
            "1 epochs completed in 0.006 hours.\n",
            "Optimizer stripped from runs/train/exp3/weights/last.pt, 14.3MB\n",
            "Optimizer stripped from runs/train/exp3/weights/best.pt, 14.3MB\n",
            "\n",
            "Validating runs/train/exp3/weights/best.pt...\n",
            "Fusing layers... \n",
            "Model summary: 157 layers, 7047883 parameters, 0 gradients, 15.9 GFLOPs\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0% 0/4 [00:00<?, ?it/s]WARNING ⚠️ NMS time limit 3.700s exceeded\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  25% 1/4 [00:04<00:12,  4.03s/it]WARNING ⚠️ NMS time limit 3.700s exceeded\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50% 2/4 [00:08<00:08,  4.10s/it]WARNING ⚠️ NMS time limit 3.700s exceeded\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:12<00:00,  3.18s/it]\n",
            "                   all        193        472     0.0008       0.06   0.000843   0.000254\n",
            "            RazorBlade        193         83          0          0          0          0\n",
            "             SafetyPin        193         23          0          0          0          0\n",
            "             PaperClip        193         89   0.000309     0.0449   0.000162   9.22e-05\n",
            "                   Pen        193         77    0.00393      0.325    0.00503    0.00157\n",
            "              ThinNail        193         10          0          0          0          0\n",
            "                 Screw        193        100          0          0          0          0\n",
            "               HandGun        193         20          0          0          0          0\n",
            "                 Knife        193         20          0          0          0          0\n",
            "     MultiPurposeKnife        193         19          0          0          0          0\n",
            "                   Key        193         10          0          0          0          0\n",
            "                 Plier        193          1          0          0          0          0\n",
            "              Shuriken        193         20    0.00536       0.35    0.00492    0.00138\n",
            "Results saved to \u001b[1mruns/train/exp3\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training monitoring"
      ],
      "metadata": {
        "id": "-Bo26seKv0ad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#To visualize the training on tensorboard dev\n",
        "#logdir = precise the path to the result of the run\n",
        "#name = name of your experiment\n",
        "#description = your description\n",
        "#one_shot = means that it stops the workflow once everything is uploaded\n",
        "\n",
        "!tensorboard dev upload --logdir runs/train \\\n",
        "  --name \"YOLOv5\" \\\n",
        "  --description \"test1\" \\\n",
        "  --one_shot"
      ],
      "metadata": {
        "id": "osb3sC4d1RTW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}